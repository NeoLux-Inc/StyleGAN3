{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/NeoLux-Inc/StyleGAN3/blob/main/StyleGAN3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%cd /content/drive/MyDrive/StyleGAN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
    "#!echo \"Shinji0923\" | sudo -S unzip ninja-linux.zip -d /usr/local/bin/\n",
    "#!echo \"Shinji0923\" | sudo -S update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kawa/AI/GitHub_public/StyleGAN3'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%cd ..\n",
    "#!git clone https://github.com/adamian98/pulse.git\n",
    "#%cd StyleGAN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import dlib\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "import dnnlib\n",
    "import legacy\n",
    "import imageio\n",
    "from tqdm.notebook import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from moviepy.video.fx.resize import resize\n",
    "from moviepy.editor import VideoFileClip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mixingを行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'ffhq_dataset'\n",
      "/home/kawa/AI/GitHub_public/StyleGAN3\n",
      "--2023-10-02 15:04:19--  https://api.ngc.nvidia.com/v2/models/org/nvidia/team/research/stylegan3/1/files?redirect=true&path=stylegan3-r-ffhq-1024x1024.pkl\n",
      "api.ngc.nvidia.com (api.ngc.nvidia.com) をDNSに問いあわせています... 54.200.153.123, 54.185.196.33\n",
      "api.ngc.nvidia.com (api.ngc.nvidia.com)|54.200.153.123|:443 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 302 Found\n",
      "場所: https://prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com/org/nvidia/team/research/models/stylegan3/versions/1/files/stylegan3-r-ffhq-1024x1024.pkl?response-content-disposition=attachment%3B%20filename%3D%22stylegan3-r-ffhq-1024x1024.pkl%22&response-content-type=application%2Foctet-stream&versionId=lkcAsTpfp64DMXIoYIvrK4pRofRRg8F8&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20231002T060420Z&X-Amz-SignedHeaders=host&X-Amz-Expires=86400&X-Amz-Credential=AKIA3PSNVSIZ42OUKYPX%2F20231002%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=143c32e4eb2fd0114cecebe24ae848dffb678fd14cee348d46df977a99c8f493 [続く]\n",
      "--2023-10-02 15:04:20--  https://prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com/org/nvidia/team/research/models/stylegan3/versions/1/files/stylegan3-r-ffhq-1024x1024.pkl?response-content-disposition=attachment%3B%20filename%3D%22stylegan3-r-ffhq-1024x1024.pkl%22&response-content-type=application%2Foctet-stream&versionId=lkcAsTpfp64DMXIoYIvrK4pRofRRg8F8&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20231002T060420Z&X-Amz-SignedHeaders=host&X-Amz-Expires=86400&X-Amz-Credential=AKIA3PSNVSIZ42OUKYPX%2F20231002%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=143c32e4eb2fd0114cecebe24ae848dffb678fd14cee348d46df977a99c8f493\n",
      "prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com (prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com) をDNSに問いあわせています... 52.218.180.249, 3.5.76.120, 3.5.83.163, ...\n",
      "prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com (prod-model-registry-ngc-bucket.s3.us-west-2.amazonaws.com)|52.218.180.249|:443 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 237040112 (226M) [application/octet-stream]\n",
      "‘stylegan3-r-ffhq-1024x1024.pklz’ に保存中\n",
      "\n",
      "stylegan3-r-ffhq-10 100%[===================>] 226.06M  2.64MB/s    in 3m 32s  \n",
      "\n",
      "2023-10-02 15:07:53 (1.06 MB/s) - ‘stylegan3-r-ffhq-1024x1024.pklz’ へ保存完了 [237040112/237040112]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%cd ffhq-dataset\n",
    "#!wget --content-disposition 'https://api.ngc.nvidia.com/v2/models/org/nvidia/team/research/stylegan3/1/files?redirect=true&path=stylegan3-r-ffhq-1024x1024.pkl' -O stylegan3-r-ffhq-1024x1024.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "NETWORK = \"ffhq-dataset/stylegan3-t-ffhq-1024x1024.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee94168268af4f3798f9b86f9ebd5f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# morphing画像を保存する\n",
    "STEPS = 5\n",
    "\n",
    "lvec1 = np.load('models/male/22_t/projected_w.npz')['w']\n",
    "lvec2 = np.load('models/male/23_0/projected_w.npz')['w']\n",
    "\n",
    "device = torch.device('cuda')\n",
    "with dnnlib.util.open_url(NETWORK) as fp:\n",
    "    G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device) # type: ignore\n",
    "\n",
    "diff = lvec2 - lvec1\n",
    "step = diff / STEPS\n",
    "current = lvec1.copy()\n",
    "target_uint8 = np.array([1024,1024,3], dtype=np.uint8)\n",
    "\n",
    "for j in tqdm(range(STEPS)):\n",
    "    z = torch.from_numpy(current).to(device)\n",
    "    synth_image = G.synthesis(z, noise_mode='const')\n",
    "    synth_image = (synth_image + 1) * (255/2)\n",
    "    synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "    # synth_imageをOpenCVの形式に変換 (BGR形式)\n",
    "    synth_image_cv = cv2.cvtColor(synth_image, cv2.COLOR_RGB2BGR)\n",
    "    resized_image = cv2.resize(synth_image_cv, (150, 150))\n",
    "    if j is 0:\n",
    "        concatenated_image = resized_image\n",
    "    else:\n",
    "        concatenated_image = np.concatenate((concatenated_image, resized_image), axis=1)\n",
    "    current = current + step\n",
    "\n",
    "# 画像を保存するパス\n",
    "image_name = 'image' + datetime.now().strftime(\"%Y%m%d_%H%M%S_%f.jpg\")\n",
    "image_path = 'outputs/mixing/' + image_name\n",
    "# OpenCVを使って画像を保存します\n",
    "cv2.imwrite(image_path, concatenated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9d0065aadc4e2fbffdf63751fe2039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# morphing動画を保存する\n",
    "# STEPS = 30で実行時間は6秒、gif動画と違ってサイズは小さく319kB：有料バージョンの上限\n",
    "STEPS = 30\n",
    "\n",
    "lvec1 = np.load('models/male/22_t/projected_w.npz')['w']\n",
    "lvec2 = np.load('models/male/23_0/projected_w.npz')['w']\n",
    "\n",
    "device = torch.device('cuda')\n",
    "with dnnlib.util.open_url(NETWORK) as fp:\n",
    "    G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device) # type: ignore\n",
    "\n",
    "diff = lvec2 - lvec1\n",
    "step = diff / STEPS\n",
    "current = lvec1.copy()\n",
    "target_uint8 = np.array([1024,1024,3], dtype=np.uint8)\n",
    "\n",
    "# 連結した画像を保存するパス\n",
    "output_path = 'outputs/mixing/video.mp4'\n",
    "\n",
    "# VideoWriterを初期化\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # H.264形式の場合\n",
    "image_width= 512\n",
    "image_height= 512\n",
    "fps = 5\n",
    "video_writer = cv2.VideoWriter(output_path, fourcc, fps, (image_width, image_height))\n",
    "\n",
    "for j in tqdm(range(STEPS)):\n",
    "    z = torch.from_numpy(current).to(device)\n",
    "    synth_image = G.synthesis(z, noise_mode='const')\n",
    "    synth_image = (synth_image + 1) * (255/2)\n",
    "    synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "    # synth_imageをOpenCVの形式に変換 (BGR形式)\n",
    "    synth_image_cv = cv2.cvtColor(synth_image, cv2.COLOR_RGB2BGR)\n",
    "    resized_image = cv2.resize(synth_image_cv, (image_width, image_height))\n",
    "    if j == 0 or j==STEPS-1:\n",
    "        for k in range(10):\n",
    "            video_writer.write(resized_image)\n",
    "    video_writer.write(resized_image)\n",
    "    current = current + step\n",
    "\n",
    "# リソースを解放\n",
    "video_writer.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dd7b86fded046f29f5f40223e9078fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# morphingのgif画像を保存する\n",
    "STEPS = 30\n",
    "\n",
    "lvec1 = np.load('models/male/22_t/projected_w.npz')['w']\n",
    "lvec2 = np.load('models/male/23_0/projected_w.npz')['w']\n",
    "\n",
    "device = torch.device('cuda')\n",
    "with dnnlib.util.open_url(NETWORK) as fp:\n",
    "    G = legacy.load_network_pkl(fp)['G_ema'].requires_grad_(False).to(device) # type: ignore\n",
    "\n",
    "diff = lvec2 - lvec1\n",
    "step = diff / STEPS\n",
    "current = lvec1.copy()\n",
    "target_uint8 = np.array([1024,1024,3], dtype=np.uint8)\n",
    "\n",
    "# 連結した画像を保存するパス\n",
    "output_path = 'outputs/mixing/animation.gif'\n",
    "\n",
    "# 画像サイズ\n",
    "image_width= 512\n",
    "image_height= 512\n",
    "\n",
    "images = []\n",
    "for j in tqdm(range(STEPS)):\n",
    "    z = torch.from_numpy(current).to(device)\n",
    "    synth_image = G.synthesis(z, noise_mode='const')\n",
    "    synth_image = (synth_image + 1) * (255/2)\n",
    "    synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n",
    "    # synth_imageをOpenCVの形式に変換 (BGR形式)\n",
    "    synth_image_cv = cv2.cvtColor(synth_image, cv2.COLOR_RGB2BGR)\n",
    "    resized_image = cv2.resize(synth_image_cv, (image_width, image_height))\n",
    "    images.append(Image.fromarray(cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB)))\n",
    "    current = current + step\n",
    "\n",
    "# GIFアニメーションを作成する\n",
    "images[0].save(output_path, save_all=True, append_images=images[1:], loop=0, duration=200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env3.10.12",
   "language": "python",
   "name": "env3.10.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
